{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMEN NLP \n",
    "\n",
    "### IA SCHOOL : 2020-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Prénoms       |     Nom         |   \n",
    "| ------------- |: -------------: |\n",
    "| Amadou lamarana      | DIALLO               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETPAE 3: CLASSIFIEURS BINAIRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous allons dans cette partie mettre en plca des classifieurs binaires sur les données d'entraiement et de test issues des 3 méthodes précédentes..\n",
    "* TF-IDF\n",
    "* Word2Vec\n",
    "* CAMEMBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETAPE 3: MISE EN PLACE DES MODÈLES DE CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import feature_extraction, model_selection, pipeline,metrics,  manifold, preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "\n",
    "## TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "##Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "### CAMEMBERT\n",
    "import torch\n",
    "from transformers import CamembertTokenizer,CamembertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "#### camembert\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset issu du scraping sur Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentaires_raw = pd.read_csv('commentaires_nettoyes_cinq_blessures_ok.csv')\n",
    "commentaires_raw = commentaires_raw.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = commentaires_raw['label'].tolist()\n",
    "X = commentaires_raw['commentaire'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthode utilisant TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pipeline\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 4))\n",
    "X_tfidf = vectorizer.fit_transform(commentaires_raw['commentaire'])\n",
    "\n",
    "## split des données\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.4, random_state=43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mat=pd.DataFrame(X_tfidf.toarray(),columns=vectorizer.get_feature_names())\n",
    "Mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASSIFIEUR NAIVE BAYES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****Nous notons une accuracy de 60%  et une fort taux de faux négatif****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n",
      "[[64 36]\n",
      " [16 84]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.64      0.71       100\n",
      "           1       0.70      0.84      0.76       100\n",
      "\n",
      "    accuracy                           0.74       200\n",
      "   macro avg       0.75      0.74      0.74       200\n",
      "weighted avg       0.75      0.74      0.74       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_nb = MultinomialNB()\n",
    "## train classifier\n",
    "clf_nb.fit(X_train_tfidf, y_train)\n",
    "preds = clf_nb.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_test, preds))\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGRESSION LOGISTIQUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Nous notons une accuracy de 66%  et une fort taux de faux négatif****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78\n",
      "[[75 25]\n",
      " [19 81]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.75      0.77       100\n",
      "           1       0.76      0.81      0.79       100\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.78      0.78      0.78       200\n",
      "weighted avg       0.78      0.78      0.78       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "lr.score(X_test_tfidf, y_test)\n",
    "preds = lr.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_test, preds))\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Les méthodes classiques de classification ne semblent pas donner d'excellents résultats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METHODE  WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split des vecteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous chargeons notre modèle déja implémenté à la précédente étape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentaires_raw= pd.read_csv('commentaires_nettoyes_cinq_blessures_ok.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wv, X_test_wv, y_train_wv, y_test_wv = train_test_split(commentaires_raw[['commentaire']], commentaires_raw['label'], test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On fait la somme des vecteurs associés aux mots qui la composent ou pas si le mot n’est pas dans le vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def recuperer_vecteur(word, model):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except KeyError:\n",
    "        return numpy.zeros((model.vector_size,))\n",
    "\n",
    "def somme_vecteurs(phrase, model):\n",
    "    return sum(recuperer_vecteur(w, model) for w in phrase)\n",
    "\n",
    "def word2vec_features(X, model):\n",
    "    feats = numpy.vstack([somme_vecteurs(p, model) for p in X])\n",
    "    return feats\n",
    "\n",
    "wv_train_feat = word2vec_features(X_train_wv['commentaire'], model_wv)\n",
    "\n",
    "wv_train_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(), TfidfTransformer())\n",
    "pipe.fit(X_train_wv['commentaire'])\n",
    "feat_train = pipe.transform(X_train_wv)\n",
    "feat_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrainement du modèle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfwv = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "clfwv.fit(wv_train_feat, y_train_wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Données de test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_test_feat = word2vec_features(X_test_wv[\"commentaire\"], model_wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation du modèle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_test_feat = word2vec_features(X_test_wv[\"commentaire\"], model_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On peut constater que la fonction de similarités ne retourne pas des résultat très intéressants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Un mauvais score 47%  avec le model word2Vec, il peut être améliorer avec une réduction des composantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47\n",
      "[[ 0 53]\n",
      " [ 0 47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        53\n",
      "           1       0.47      1.00      0.64        47\n",
      "\n",
      "    accuracy                           0.47       100\n",
      "   macro avg       0.23      0.50      0.32       100\n",
      "weighted avg       0.22      0.47      0.30       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clfwv.predict(wv_test_feat)\n",
    "print(accuracy_score(y_test_wv, y_pred))\n",
    "print(confusion_matrix(y_test_wv, y_pred))\n",
    "print(classification_report(y_test_wv, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réduction de la dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "pipe_svd = make_pipeline(CountVectorizer(), TruncatedSVD(n_components=300))\n",
    "pipe_svd.fit(X_train_wv['commentaire'])\n",
    "feat_train_svd = pipe_svd.transform(X_train_wv['commentaire'])\n",
    "feat_test_svd = pipe_svd.transform(X_test_wv['commentaire'])\n",
    "feat_train_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svd = RandomForestClassifier(n_estimators=200)\n",
    "clf_svd.fit(feat_train_svd, y_train_wv)\n",
    "y_pred = clf_svd.predict(feat_test_svd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feat_test_svd = pipe_svd.transform(X_test_wv['commentaire'])\n",
    "clf_svd.score(feat_test_svd, y_test_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La performance améliore avec 60% mais faiblement par rapport  à la méthode sans l'ACP. Il faudrait sans doute jouer avec les hyperparamètres de l’apprentissage ou réutiliser un model appris sur un corpus similaire aux données initiales mais nettement plus grand. On peut constater que la fonction de similarités ne retourne pas des résultat très intéressants**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELE AVEC CAMEMBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite nous allons importer le modèle, le tokenizer et les weights pré-entraînés. Les weights sont comme les word embeddings. Deux choses à noter :\n",
    "\n",
    "Bert a son propre tokenizer avec un vocabulaire fixe. Il est donc inutile de tokénisez vous-même.\n",
    "\n",
    "Ces weights sont issus du modèle à l’état “brut”.\n",
    "\n",
    "En pratique vous devez fine-tuner Camembert sur des données à vous afin d’avoir des weights propres à chaque tâche et corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model, tokenizer and weights\n",
    "camembert, tokenizer, weights = (ppb.CamembertModel, ppb.CamembertTokenizer, 'camembert/camembert-large')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer.from_pretrained(weights)\n",
    "model = camembert.from_pretrained(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = commentaires_raw['commentaire']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert ne sait que tokéniser des phrases de longueur maximale de 512 tokens. Ici nous allons simplement enlever les commentaires trop longs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Le commentaire le plus long est de: 115 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if there are length > 512\n",
    "max_len = 0\n",
    "for i,sent in enumerate(comments):\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    if len(input_ids) > 512:\n",
    "        print(\"annoying review at\", i,\"with length\",\n",
    "              len(input_ids))\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que la phrase la plus longue enlevée, nous faisons un padding de 115 tokens pour homogénéiser la longueur de phrases. Cela rendra l’entraînement plus simple. Nous indiquons aussi où se trouve les paddings avec np.where pour que Bert sache traiter les tokens de padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = comments.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin nous transformons les tokens en tensor pour les passer dans le fameux transformer. Seule la dernière couche est conservée pour faire la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)\n",
    "attention_mask = torch.tensor(attention_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entraîner un modèle logistique\n",
    "\n",
    "Comme nous avons seulement besoin du premier token (CLS qui signifie classification) pour le modèle logistique, nous faisons en slice avec [:,0,:]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "labels = commentaires_raw['label']\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous commençons par faire un split train/test avec Scikit-Learn. Ensuite nous utilisons Grid Search pour essayer de trouver le meilleur paramètre. Finalement on entraîne le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:  {'C': 15.790736842105265, 'kernel': 'rbf'}\n",
      "best scrores:  0.7226666666666667\n"
     ]
    }
   ],
   "source": [
    "## grid search\n",
    "# Grid search\n",
    "from sklearn.svm import SVC\n",
    "#parameters = {'C': np.linspace(0.00, 100, 20)}\n",
    "parameters_svc = {'kernel':('linear', 'rbf'), 'C':np.linspace(0.0015, 100, 20)}\n",
    "grid_search = GridSearchCV(SVC(), parameters_svc)\n",
    "#grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(C=grid_search.best_params_['C'])\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "y_pred = lr_clf.predict(test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous arrivons donc à un score de 72%, qui est un grand mais toujours faible.. Cela peut s'expliquer pas les mauvaises par la notation (note attribuée) humaine qui peut ne pas très fiable au caracteres positif ou négatif du commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.score(test_features, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n",
      "[[46 18]\n",
      " [22 39]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70        64\n",
      "           1       0.68      0.64      0.66        61\n",
      "\n",
      "    accuracy                           0.68       125\n",
      "   macro avg       0.68      0.68      0.68       125\n",
      "weighted avg       0.68      0.68      0.68       125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_labels, y_pred))\n",
    "print(confusion_matrix(test_labels, y_pred))\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION PARTIELLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nous avons tenter des classifier des commentaires des appréciations des produits sur Amazons.. \n",
    "#### Nous notons que les classifieurs utilisés donnent des resulalts mitigés voir pas du tout bon. \n",
    "#### Ce pendant la puissance de ses algorithmes sur les méthodes de classificatios prouvées, nous laisse croire que les notations (attribution des étoiles)des commentaires faits par l'humain ne sont pas toujours pas conforme avec le commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_wv, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
