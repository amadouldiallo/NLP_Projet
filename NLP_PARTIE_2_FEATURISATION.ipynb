{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMEN NLP \n",
    "\n",
    "### IA SCHOOL : 2020-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Prénoms       |     Nom         |   \n",
    "| ------------- |: -------------: |\n",
    "| Amadou lamarana      | DIALLO               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETAPE 2: FEATURISATION DES COMMENTAIRES\n",
    "\n",
    "#### Nous allons utiliser TF-IDF qui est une méthode de pondération souvent utilisée en recherche d'information et en particulier dans la fouille de textes. Cette mesure statistique permet d'évaluer l'importance d'un terme contenu dans un document, relativement à une collection ou un corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import pickle\n",
    "## TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "##Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "### CAMEMBERT\n",
    "import torch\n",
    "from transformers import CamembertTokenizer,CamembertModel\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from python-Levenshtein) (52.0.0.post20210125)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp38-cp38-macosx_10_9_x86_64.whl size=80635 sha256=a0668b1099e27a1d79eeaf519a8181edcbf2b7aa4f0f24fbca69f82b9dd35272\n",
      "  Stored in directory: /Users/amadoudiallo/Library/Caches/pip/wheels/d7/0c/76/042b46eb0df65c3ccd0338f791210c55ab79d209bcc269e2c7\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers==3.3.1\n",
    "#!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Recuperer le dataset des commentaires nettoyés après l'étape 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentaires_raw = pd.read_csv('commentaires_nettoyes_cinq_blessures_ok.csv')\n",
    "commentaires_raw = commentaires_raw.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dans le cadre de la création du modèle à la partie 3 nous allons scinder nos données en train et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = commentaires_raw['label'].tolist()\n",
    "X = commentaires_raw['commentaire'].tolist()\n",
    "commentaires_raw[commentaires_raw.label==1]['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nous allons effectuer la featurisation , c'est à dire transformer les mots en des valeurs numériques.. compréhensible par la machine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) TF-IDF \n",
    "\n",
    "Cette méthode qui permet d'indentifier les mots du corpus très importants sur un document et pas sur les autres documents. \n",
    "Les features_names seront regroupé de 1 à 4 mots .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Nous allons constitué le commentaire par un ensemble de groupe de mot de 2 à 4, afin de pouvoir comprendre le contexte \n",
    "\"\"\"\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 4))\n",
    "train_tfid = vectorizer.fit_transform(X)\n",
    "##\n",
    "X_train_tfid, X_test_tfidf, y_train, y_test = train_test_split(train_tfid, y, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Je vais maintenant utiliser le vectorizer sur le corpus prétraité de l'ensemble des commentaires pour extraire un vocabulaire et créer la matrice de caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mat=pd.DataFrame(train_tfid.toarray(),columns=vectorizer.get_feature_names())\n",
    "Mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons mettre nos commentaires sous forme de vecteurs numpy.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### le dictionnaire des vocabulaires avec les index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Word2Vec\n",
    "\n",
    "<hr>\n",
    "Je vais ajuster mon propre modèle Word2Vec sur le corpus de données d'entraînement avec gensim. Avant d'ajuster le modèle, le corpus doit être transformé en une liste de listes de n-grams.\n",
    "<hr>\n",
    "la taille cible des vecteurs de mots, j'utiliserai 200 ;\n",
    "la fenêtre, ou la distance maximale entre le mot actuel et le mot prédit dans une phrase, j'utiliserai la longueur moyenne du texte dans le corpus ;\n",
    "l'algorithme d'apprentissage, j'utiliserai le skip-grammes (sg=1) car en général il donne de meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = [elm.split() for elm in X] ## transformer les données d'entrainement en une liste de liste/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creer le modèle word2Vec.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv = Word2Vec(sentences=liste, vector_size=200, window=10, min_count=2, workers=8,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### le nombre de corpus du document.. ( 500 commentaires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv.corpus_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on enregistre le modèle sous le nom \"word2vec.model\" que nous utiliserons pour la prochaine étape sur la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv.wv.get_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On peut afficher les textes avec leurs représentaions numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Boucle des 20 premiers words et leurs vecteurs..créés par Word2Vec\n",
    "\"\"\"\n",
    "i = 0\n",
    "for elm in model_wv.wv.index_to_key:\n",
    "    if i < 30:\n",
    "        print(elm)\n",
    "        print(model_wv.wv.similar_by_word(elm))\n",
    "        print(\"----------\")\n",
    "        print(model_wv.wv.get_vector(elm, norm=True))\n",
    "        \n",
    "        #print(model_wv[elm])\n",
    "        print('________')\n",
    "        i= i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nett(tex):\n",
    "    \"\"\"\n",
    "    input: texte à nettoyer\n",
    "    output: texte nettoyer de stop_words \n",
    "    \"\"\"\n",
    "    # Conserver tous les caracteres sauf les speciaux ($¨#([)) et le chiffres\n",
    "    p='[a-zA-Zéâûèàùîôêç\\-.!?:]{1,}'\n",
    "    \n",
    "    # stop words\n",
    "    with open('mes_stop_words.txt') as f:\n",
    "        s_w=f.read().split('\\n')\n",
    "    s_w.remove('fait')\n",
    "    \n",
    "    # Laisser un peu de place autour de chaque separateur de phrase\n",
    "    tex=tex.replace('!',' ! ').replace('.',' . ').replace('?',' ? ').replace(':',' : ')\n",
    "    \n",
    "    final=[]\n",
    "    for mot in re.findall(p,tex):\n",
    "        if mot.lower() not in s_w :\n",
    "            final.append(mot.lower())\n",
    "    return ' '.join(final).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarité des mots detecté par le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv.wv.most_similar(nett('livre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nous notons une mauvaise similiarité qui risque de causer préjudice au modèle avec Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv.wv['lire']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Camembert (Le BERT français)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons réaliser la featurization en utilisant Camembert .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1) récuperation du dataset et chargement du tokenizer de Camembert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du jeu de donnees\n",
    "dataset = pd.read_csv(\"commentaires_nettoyes_cinq_blessures_ok.csv\")\n",
    " \n",
    "commentaires = dataset['commentaire'].values.tolist()\n",
    "labels = dataset['label'].values.tolist()\n",
    " \n",
    "# On charge l'objet \"tokenizer\"de camemBERT qui va servir a encoder\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert/camembert-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2) Création du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_camb = CamembertModel.from_pretrained('camembert/camembert-base',output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_camb.state_dict(), \"./model_camb.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on pourra le charger au plus tard avec la méthode load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"model_camb.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3) Featuratisation des commentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La fonction batch_encode_plus encode un batch de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## on encode les commentaires\n",
    "encoded = tokenizer.batch_encode_plus(commentaires,\n",
    "                                            add_special_tokens=True,\n",
    "                                            padding=True,\n",
    "                                            truncation=True,\n",
    "                                            return_attention_mask = True,\n",
    "                                            return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4) On transforme la liste des labels en tenseur\n",
    " #### On transforme la liste des sentiments en tenseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"acheter ce livre est une pire perte\",return_tensors='pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model_camb(input_ids)\n",
    "outputs[2][-4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nous avons pu featurisé nos données. la suite est la partie 3 qui implémente les modèles avec les 3 méthodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
